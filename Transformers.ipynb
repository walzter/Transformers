{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "## C = A @ B\n",
    "## [k, m, p] = [k, m, n] @ [k, n, p]\n",
    "## assuming k = batchsize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention Mechanism\n",
    "def attention(query, key, value, mask=None, dropout=None,d_k:int=None, verbosity:bool=True):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    ## reshape the scores to (batch_size, seq_len, d_k) with einsum function\n",
    "    scores = torch.einsum('bik,bjk->bij', query, key) / math.sqrt(d_k)\n",
    "    if verbosity:\n",
    "        print(\"==>Actual Attention Mechanism<==\")\n",
    "        ## print the mask size \n",
    "        print(f\"q size:     \\t{query.size()}\")\n",
    "        print(f\"k size:     \\t{key.size()}\")\n",
    "        print(f\"v size:     \\t{value.size()}\")\n",
    "        print(f\"mask size:  \\t{mask.size()}\")\n",
    "        print(f\"d_k:        \\t{d_k}\")\n",
    "        ## print the size of the scores \n",
    "        print(f\"scores size:\\t{scores.size()}\")\n",
    "        print(\"==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\")\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    print(f\"p_attn size:\\t{p_attn.size()}\")\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "## write a function to test the attention mechanism\n",
    "def test_attention(batch_size:int=2, seq_len:int=3, d_model:int=4, n_heads:int=2, verbosity:bool=True):\n",
    "    d_k = d_v = d_model // n_heads\n",
    "    # (batch_size, seq_len, d_model)\n",
    "    q = torch.randn(batch_size, seq_len, d_model)\n",
    "    # (batch_size, seq_len, d_model)\n",
    "    k = torch.randn(batch_size, seq_len, d_model)\n",
    "    # (batch_size, seq_len, d_model)\n",
    "    v = torch.randn(batch_size, seq_len, d_model)\n",
    "    # (batch_size, seq_len, seq_len)\n",
    "    mask = torch.ones(batch_size, seq_len, d_model)\n",
    "    mask[:,:,0] = 0\n",
    "    mask[:,:,2] = 0\n",
    "    # (batch_size, seq_len, d_model)\n",
    "    if verbosity:\n",
    "        print(\"==>Testing Attention Mechanism<==\")\n",
    "        print(f\"q size:   \\t{q.size()}\")\n",
    "        print(f\"k size:   \\t{k.size()}\")\n",
    "        print(f\"v size:   \\t{v.size()}\")\n",
    "        print(f\"mask size:\\t{mask.size()}\")\n",
    "        print(f\"d_k:      \\t{d_k}\")\n",
    "        print(\"==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\\n\")\n",
    "    output, attn = attention(q, k, v, mask=mask,d_k=d_k, verbosity=verbosity)\n",
    "    # print the size of the output and the output\n",
    "    print(f\"output size:\\t{output.size()}\")\n",
    "    print(f\"attn size:\\t{attn.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Testing Attention Mechanism<==\n",
      "q size:   \ttorch.Size([5, 3, 3])\n",
      "k size:   \ttorch.Size([5, 3, 3])\n",
      "v size:   \ttorch.Size([5, 3, 3])\n",
      "mask size:\ttorch.Size([5, 3, 3])\n",
      "d_k:      \t1\n",
      "==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\n",
      "\n",
      "==>Actual Attention Mechanism<==\n",
      "q size:     \ttorch.Size([5, 3, 3])\n",
      "k size:     \ttorch.Size([5, 3, 3])\n",
      "v size:     \ttorch.Size([5, 3, 3])\n",
      "mask size:  \ttorch.Size([5, 3, 3])\n",
      "d_k:        \t3\n",
      "scores size:\ttorch.Size([5, 3, 3])\n",
      "==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\n",
      "p_attn size:\ttorch.Size([5, 3, 3])\n",
      "output size:\ttorch.Size([5, 3, 3])\n",
      "attn size:\ttorch.Size([5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "test_attention(batch_size = 5, seq_len=3, d_model=3, n_heads=3, verbosity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the class for the Multi-Head Attention Mechansim in a Transformer \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the Multi-Headed Attention Mechanism, defining the: \n",
    "    number of heads : n_heads \n",
    "    output dimension: out_dim \n",
    "    dropout rate    : dropout\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.h = n_heads\n",
    "        self.linears = self.clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    ## define clones \n",
    "    def clones(self, module, N):\n",
    "        \"Produce N identical layers.\"\n",
    "        return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "    ## define the split_heads method\n",
    "    def split_heads(self, x, n_heads, d_k):\n",
    "        \"Split x into different heads, with dimension d_k.\"\n",
    "        return x.view(n_heads, -1, d_k)\n",
    "    ## define the concat method\n",
    "    def concat(self, x):\n",
    "        \"Concatenate tensors with different dimensions.\"\n",
    "        return x.transpose(0, 1).contiguous().view(-1, self.d_k * self.h)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        n_heads = self.h\n",
    "        d_k = self.d_k\n",
    "        q, k, v = self.linears[0](query), self.linears[1](key), self.linears[2](value)\n",
    "        q, k, v = self.split_heads(q, n_heads, d_k), self.split_heads(k, n_heads, d_k), self.split_heads(v, n_heads, d_k)\n",
    "        # perform attention\n",
    "        ## print the size of the q, k ,v \n",
    "        print(f\"q size:   \\t{q.size()}\")\n",
    "        print(f\"k size:   \\t{k.size()}\")\n",
    "        print(f\"v size:   \\t{v.size()}\")\n",
    "        attn, self.attn = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "        # concatenate heads\n",
    "        attn = self.concat(attn)\n",
    "        # apply final linear layer\n",
    "        attn = self.linears[3](attn)\n",
    "        return attn\n",
    "\n",
    "## test the multi-head attention mechanism\n",
    "def test_mha(n_heads, d_model, verbosity:bool=True):\n",
    "    mha = MultiHeadAttention(n_heads, d_model)\n",
    "    if verbosity:\n",
    "        print(\"==>Testing Multi-Head Attention Mechanism<==\")\n",
    "        print(f\"n_heads:   \\t{n_heads}\")\n",
    "        print(f\"d_model:   \\t{d_model}\")\n",
    "        print(f\"d_k:       \\t{mha.d_k}\")\n",
    "        print(\"==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\\n\")\n",
    "    ## define the input \n",
    "    query = torch.randn(2, 3, d_model)\n",
    "    key = torch.randn(2, 3, d_model)\n",
    "    value = torch.randn(2, 3, d_model)\n",
    "    mask = torch.ones(2, 3, d_model)\n",
    "    mask[:,:,0] = 0\n",
    "    mask[:,:,2] = 0\n",
    "    output = mha(query, key, value, mask=mask)\n",
    "    print(output.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Testing Multi-Head Attention Mechanism<==\n",
      "n_heads:   \t3\n",
      "d_model:   \t3\n",
      "d_k:       \t1\n",
      "==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\n",
      "\n",
      "q size:   \ttorch.Size([3, 6, 1])\n",
      "k size:   \ttorch.Size([3, 6, 1])\n",
      "v size:   \ttorch.Size([3, 6, 1])\n",
      "==>Actual Attention Mechanism<==\n",
      "q size:     \ttorch.Size([3, 6, 1])\n",
      "k size:     \ttorch.Size([3, 6, 1])\n",
      "v size:     \ttorch.Size([3, 6, 1])\n",
      "mask size:  \ttorch.Size([2, 1, 3, 3])\n",
      "d_k:        \t1\n",
      "scores size:\ttorch.Size([3, 6, 6])\n",
      "==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8597/1277413356.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_mha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8597/1078998077.py\u001b[0m in \u001b[0;36mtest_mha\u001b[0;34m(n_heads, d_model, verbosity)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ci_covid/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8597/1078998077.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"k size:   \\t{k.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"v size:   \\t{v.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m# concatenate heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8597/1425031132.py\u001b[0m in \u001b[0;36mattention\u001b[0;34m(query, key, value, mask, dropout, d_k, verbosity)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==>~~~~~~~~~~~~~~~~~~~~~~~~~~~<==\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mp_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"p_attn size:\\t{p_attn.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "test_mha(n_heads=3, d_model=3, verbosity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Feed-Forward Network Class \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    ## forward method \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Encoder Layer Class\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, d_model, dropout=dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout=dropout)\n",
    "        ## apply layer norm \n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        ## apply dropout \n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.drop1(self.mha(x2, x2, x2, mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.drop2(self.ffn(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Decoder Layer Class \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttention(n_heads, d_model, dropout=dropout)\n",
    "        self.mha2 = MultiHeadAttention(n_heads, d_model, dropout=dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout=dropout)\n",
    "        ## apply layer norm \n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.norm_3 = nn.LayerNorm(d_model)\n",
    "        ## apply dropout \n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "    def forward(self, x, e_outputs, src_mask, tgt_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.drop1(self.mha1(x2, x2, x2, tgt_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.drop2(self.mha2(x2, e_outputs, e_outputs, src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.drop3(self.ffn(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Positional Encoding class \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    ## forward pass \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Encoder class \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout=dropout) for _ in range(n_layers)])\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Decoder \n",
    "class Decoder(nn.Module):\n",
    "    def _init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout=dropout) for _ in range(n_layers)])\n",
    "    ## define the forward pass \n",
    "    def forward(self, trg, e_outputs, src_mask, tgt_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, e_outputs, src_mask, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Embedder \n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, src):\n",
    "        return self.embed(src) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the Transformer class using all the classes before \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, d_model, n_layers, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, n_layers, n_heads, d_ff, dropout=dropout)\n",
    "        self.decoder = Decoder(tgt_vocab, d_model, n_layers, n_heads, d_ff, dropout=dropout)\n",
    "        self.out = nn.Linear(d_model, tgt_vocab)\n",
    "    \n",
    "        ## define get_pad_mask \n",
    "    def get_pad_mask(self, x):\n",
    "        return (x == 0).unsqueeze(-2)\n",
    "    \n",
    "    ## forward pass \n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.get_pad_mask(src)\n",
    "        tgt_mask = self.get_pad_mask(trg)\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_outputs = self.decoder(trg, e_outputs, src_mask, tgt_mask)\n",
    "        output = self.out(d_outputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8597/511747045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8597/3845540752.py\u001b[0m in \u001b[0;36mtest_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0md_ff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;31m# create a test input and target sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8597/3845540752.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src_vocab, tgt_vocab, d_model, n_layers, n_heads, d_ff, dropout)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dropout'"
     ]
    }
   ],
   "source": [
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6858c3dbc49267e902ff986705b591b9d7b57befff84fd7d814fe16c4a8e1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
